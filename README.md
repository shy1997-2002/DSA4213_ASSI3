# IMDB Sentiment Classification with Parameter-Efficient Fine-Tuning (PEFT)  
  
This repository contains the code and configurations used to compare **Full Fine-tuning**, **LoRA**, and **BitFit** methods on the **IMDB sentiment analysis dataset** using the DistilBERT model.  
The goal of this project is to analyze the trade-offs between different fine-tuning strategies in terms of **performance**, **parameter efficiency**, and **computational cost**.
---  
## Project Overview  
  
This project was developed for the **DSA4213: Advanced Topics in AI Systems** course assignment.    
It demonstrates the implementation and evaluation of several **Parameter-Efficient Fine-Tuning (PEFT)** methods using Hugging Faceâ€™s `transformers` and `peft` libraries.
---  
## Install dependencies
```
pip install -r requirements.txt
```
---
## Running Experiments
### 1ï¸âƒ£ Full Fine-tuning
```
python train.py --config configs/imdb_full.yaml
```
### 2ï¸âƒ£ LoRA Fine-tuning
```
python train.py --config configs/imdb_lora.yaml
```
### 3ï¸âƒ£ BitFit Fine-tuning
```
python train.py --config configs/imdb_BitFit.yaml
```
### 4ï¸âƒ£ Evaluation
After training, run evaluation:
```
python my_evaluate.py --config configs/imdb_BitFit.yaml 
# or imdb_full.yaml / imdb_lora.yaml
```
---
## Results
The following table summarizes the evaluation results of three fine-tuning strategies (Full, LoRA, BitFit) on the **IMDB sentiment classification dataset** using **DistilBERT-base-uncased**.

| Method               | Trainable Params | Eval Loss  |  Accuracy  |  F1 Score  | Notes                                |
| :------------------- | :--------------: | :--------: | :--------: | :--------: | :----------------------------------- |
| **Full Fine-tuning** |    66M (100%)    |   0.3487   | **0.9166** | **0.9565** | Highest accuracy but most parameters |
| **LoRA**             |    â‰ˆ3M (~5%)     | **0.2645** |   0.8948   |   0.9445   | Strong performance with 5% params    |
| **BitFit**           |   â‰ˆ0.7M (~1%)    |   0.3214   |   0.8670   |   0.9288   | Lightest method, lower accuracy      |

---
## ğŸ“ Repository Structure
```
F:/NUS_AIS/DSA4213/3_assignment/  
â”œâ”€â”€ configs/  
â”‚ â”œâ”€â”€ imdb_adapter.yaml # Adapter configuration
â”‚ â”œâ”€â”€ imdb_BitFit.yaml # BitFit configuration  
â”‚ â”œâ”€â”€ imdb_full.yaml # Full fine-tuning configuration  
â”‚ â”œâ”€â”€ imdb_lora.yaml # LoRA configuration  
â”‚ â””â”€â”€ imdb_prompt.yaml # Prompt-tuning configuration  
â”‚  
â”œâ”€â”€ logs/  
â”‚ â””â”€â”€ eval/  
â”‚ â””â”€â”€ runs/ # Evaluation logs  
â”‚ â”œâ”€â”€ Oct06_10-31-07_shy-personal-PC  
â”‚ â”œâ”€â”€ Oct06_13-26-12_shy-personal-PC  
â”‚ â””â”€â”€ Oct06_13-42-45_shy-personal-PC  
â”‚  
â”œâ”€â”€ outputs/  
â”‚ â”œâ”€â”€ imdb_BitFit/ # BitFit fine-tuned models outputs  
â”‚ â”œâ”€â”€ imdb_full/ # Full fine-tuned models outputs  
â”‚ â””â”€â”€ imdb_lora/ # LoRA fine-tuned models outputs  
â”‚  
â”œâ”€â”€ data_prep.py # Data preprocessing script  
â”œâ”€â”€ evaluate.py # Official evaluation script  
â”œâ”€â”€ my_evaluate.py # Custom evaluation script  
â”œâ”€â”€ train.py # Main training script  
â”œâ”€â”€ requirements.txt # Python dependencies  
â””â”€â”€ method.txt # methods description and run commands
```
---
Note: the outputs/ directory (training outputs and fineâ€‘tuned model weights) is not included in this repository. To avoid inflating the repository and because Git is not suitable for storing large binary files, we do not upload model checkpoints, generated outputs, or full datasets to GitHub. Run the training script locally to create the outputs directory and save the trained models 

(for example: python train.py --config configs/imdb_lora.yaml).
